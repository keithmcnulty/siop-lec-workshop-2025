---
title: "Survival Analysis Exercises"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In this document we will conduct survival analysis on a data set relating employee turnover to several variables that were measured at the time an employee was hired. 

## Exercise 1 - Load, explore, and structure the survival data

```{r}
library(dplyr)
library(survival)
library(survminer)
library(lubridate)

# Load the survival analysis data file from the url provided
url <- "https://rstudio-conf-2022.github.io/people-analytics-rstats/data/survival_analysis_data.csv"
surv_data <- read.csv(url)
```

```{r}
# view the first few rows in the data set to acquaint yourself with the data
head(surv_data)

# convert columns ending in 'DATE' to date format
surv_data <- surv_data |> 
  dplyr::mutate(across(ends_with("DATE"), as.Date))
```

```{r}
# Create a censor indicator variable that is 1 if the employee has left and 0 otherwise
surv_data <- 
  surv_data |> 
  dplyr::mutate(
    CENSOR = dplyr::case_when(
      TURNOVER == "Yes" ~ 1,
      TRUE ~ 0
    )
  )
```

```{r}
# Replace missing TURNOVER_DATE values with "2022-07-01" -- think about what missing TURNOVER_DATES mean
surv_data <- 
  surv_data |>
  dplyr::mutate(
    TURNOVER_DATE = dplyr::case_when(
      is.na(TURNOVER_DATE) ~ as.Date("2022-07-01"),
      TRUE ~ TURNOVER_DATE
    )
  )
```

```{r}
# Use the lubridate::interval() function to create the EVENT_TIME variable 
# Hint: Use  %/% months(1) to transform the time difference into months 
surv_data <- 
  surv_data |>
  dplyr::mutate(
    EVENT_TIME = lubridate::interval(HIRE_DATE, TURNOVER_DATE) %/% months(1)
  )
```

```{r}
# Calculate some descriptive statistics for EVENT_TIME using the full sample, then group the data by CENSOR to see how the descriptives
# change. 
mean(surv_data$EVENT_TIME)
median(surv_data$EVENT_TIME)

surv_data |>
  dplyr::group_by(
    CENSOR
  ) |>
  dplyr::summarise(
    MEAN_EVENT_TIME = mean(EVENT_TIME),
    MEDIAN_EVENT_TIME = median(EVENT_TIME),
    SD_EVENT_TIME = sd(EVENT_TIME)
  )
```

## Exercise 2 - Create a survival object and estimate survival functions using Kaplan Meier estimates

```{r}
# Create a survival object using survival::Surv().
# Remember it requires two arguments EVENT_TIME and CENSOR
surv_object <- survival::Surv(
  event = surv_data$CENSOR,
  time = surv_data$EVENT_TIME
)
```

```{r}
# Use survival::survfit() to estimate survival probabilities using the Kaplan Meier estimator for the entire sample
# Save the survfit output into an object called km_all

# How many months after hiring would you expect 50% of the sample to remain at the firm? (e.g. the median survival time)
# How many months after hiring would you expect 75% of the sample to remain at the firm?

km_all <- survival::survfit(surv_object ~ 1, data = surv_data)
summary(km_all)
```

```{r}
# Look at the structure of km_all using str()
str(km_all)
```

```{r}
# Using km_all, create a new data frame called km_calc which contains the following variables from km_all:
# time, n.risk, n.censor, n.event, and surv 

km_calc <- 
  tibble::tibble(
    time = km_all$time,
    n.risk = km_all$n.risk,
    n.censor = km_all$n.censor,
    n.event = km_all$n.event,
    survival = km_all$surv
  )

```

```{r}
# Using km_calc, determine the following:
# How many turnovers occurred in the dataset?
sum(km_calc$n.event)

# How many censored observations are in the dataset?
sum(km_calc$n.censor)

# At what time point do the first censored observations appear? 
min(which(km_calc$n.censor != 0))

# Subtract n.risk at time point 2 from n.risk at time point 1.
km_calc$n.risk[1] - km_calc$n.risk[2]
# Why did n.risk decrease from time point 1 to time point 2?
# n.risk at time point 2 = n.risk at time point 1 - n.events at time point 1 - n.censor at time point 1

# Subtract n.risk at time point 56 from time point 55.
km_calc$n.risk[55] - km_calc$n.risk[56] 
# Why did n.risk decrease from time point 55 to time point 56?
# n.risk at time point 56 = n.risk at time point 55 - n.events at time point 55 - n.censor at time point 55


```


```{r}
# The formula to calculate the survival probabilities using the KM estimator is: 
# survival(t - 1) * (1 - (n.event[t] / n.risk[t])) 
# That is, the survival probability at time t is equal to the survival probability at time t-1 multiplied by 1 - (number of events at 
# time t / number of individuals at risk at time t).

# Using the formula above, manual calculate the survival probabilities and save them as variable in km_calc as surv_calc. Take a moment to think about how censored observations are used in the calculation as well.
km_calc <- 
  km_calc |>
  dplyr::mutate(
    surv_calc_1 = (1 - n.event / n.risk),
    surv_calc = cumprod(surv_calc_1)
  )
```

Median: 47 months
75: 24 months
```{r}
# Use survminer::ggsurvplot() to plot the overall survival function.
survminer::ggsurvplot(fit = km_all)
```

```{r}
# Use survival::survfit() to estimate survival probabilities by the REFERRAL variable. 
# For the portion of the sample that was referred for the job (REFERRAL == "Yes"), how many months after hiring would you expect 50% of the sample to remain at the firm? What about for the portion of the sample that was not referred? 
# Why would the median survival time be missing? 

km_ref <- survival::survfit(surv_object ~ REFERRAL, data = surv_data)
summary(km_ref)
```

Referral == "Yes"
Median is undefined as more than 50% remain after end of study.

Referral == "No"
Median: 44 months

```{r}
# Use survminer::ggsurvplot() to plot the survival function by REFERRAL status. Are the two curves different? 
survminer::ggsurvplot(km_ref)
```

```{r}
# Using the survfit function, estimate the survival probabilities for the interaction between REFERRAL and NUMBER_JOBS variables in 
# surv_data. Then plot the survival curve using ggsurvplot and provide an interpretation of the curves. Does their appear to be an
# interaction?
# 
# HINT: You don't need to create a new variable!  

km_ref_job <- survival::survfit(surv_object ~ REFERRAL + NUMBER_JOBS, data = surv_data)
survminer::ggsurvplot(km_ref_job)
```

## Exercise 3 - Fit a cox proportional hazards model to your data. 

```{r}
# Estimate a cox proportional hazards model. Include all of the main effects and the interactions between INTPER_RATE and PROG_EXER_RATE as well as HIRE_REC and REFERRAL. 

# HINT: An interaction variable is created by multiplying Variable 1 with Variable 2 (Variable 1 * Variable 2).

mod_1 <- survival::coxph(
  surv_object ~ NUMBER_JOBS + PROG_EXER_RATE + 
    INTPER_RATE + PRIOR_EXP + HIRE_REC*REFERRAL + INTPER_RATE*PROG_EXER_RATE,
  data = surv_data
)
```

```{r}
# View the coefficient estimates and standard errors of the model
summary(mod_1)
```

```{r}
# Estimate a cox proportional hazards model that only includes the main effects (no interactions).
mod_2 <- survival::coxph(
  surv_object ~ NUMBER_JOBS + PROG_EXER_RATE + 
    INTPER_RATE + PRIOR_EXP + HIRE_REC + REFERRAL,
  data = surv_data
)
```

```{r}
# View the coefficient estimates and standard errors for the main effects model. 
summary(mod_2)
```

```{r}
# Compare the model you fit above to a model that only includes the main effects (no interactions). Use the anova() function to determine if the model with interactions fits your data significantly better than the model with main effects only. If you are unsure of how to use the anova() function, then view the help documentation on it ?anova.coxph or exercise solutions.  
anova(mod_1, mod_2)
```

```{r}
# Based on the model comparison above, decide if you should choose the more complicated interaction model or the less complicated main effects model. Using the model you decided on, test the proportional hazard assumption and determine if any variables violate it. If they do decide if you should drop them from the model.
survival::cox.zph(mod_2)

mod <- survival::coxph(
  surv_object ~ REFERRAL + NUMBER_JOBS + PROG_EXER_RATE + 
    INTPER_RATE + HIRE_REC,
  data = surv_data
)
```

Prior experience is close to violating the assumption and is unrelated to attrition, so we can safely drop it from the model
## Exercise 4 - Interpreting the proportional hazards results.

```{r}
# Using the final model you decided on during last exercise, provide an interpretation of each of the predictors you kept in the model. 
summary(mod)
```
Referral reduces hazard by a factor of .38 (~62%).
No difference between 0-1 and 2 jobs prior to hiring. 
Having 3+ jobs in the year before hire increases hazard by a factor of 1.50 (50%).
Compared to a "Concerns" rating on the programming exercise, Pass and Pass+ both reduce hazard by factors of .79 (21%) and .73 (27%), respectively.
Every unit increase in interpersonal skills reduces the hazard by a factor of .91 (9%).
Every additional increase in hiring rec reduces hazard by a factor of .92 (8%).

```{r}
# Use the function confint() to calculate the 95% confidence intervals for each coefficient.
# Transform the confidence intervals so that they can be interpreted as confidence intervals for exp(coef).
confint(mod)
exp(confint(mod))
```

## Exercise 5 - Creating predicted survival curves from your proportional hazards model. 

```{r}
# Create a new data frame that contains a column for each of the variables you included in your Cox regression model. Pick of variable you are interested in and provide different values for that variable (the values need to occur in the original data frame) while holding the other variables constant (e.g. NUMBER_JOBS == "0-1" for all rows in the new data frame).
new_data <- data.frame(REFERRAL = c("No", "Yes"), 
                       NUMBER_JOBS = c("3+", "3+"), 
                       PROG_EXER_RATE = c("Concerns", "Concerns"), 
                       INTPER_RATE = c(1, 1), 
                       PRIOR_EXP = c("No", "No"), 
                       HIRE_REC = c(0, 0))
```

```{r}
# With your new data frame and your estimated Cox regression model, use the function: survival::survfit() to create predicted survival probabilities from your model and new data frame. 
# Using summary(), explore these new probabilities. 
predicted_surv_prob <- survival::survfit(mod, newdata = new_data)
summary(predicted_surv_prob)
```

```{r}
# With your new data frame and your estimated Cox regression model, use the function: survminer::ggadjustedcurves() to plot your predicted survival functions. If you are familiar with ggplot2, then try to customize the plot output. 
survminer::ggadjustedcurves(mod, 
                            data = new_data, variable = "REFERRAL", 
                            ylab = "Probability of Staying",
                            xlab = "Months Since Hire",
                            ggtheme = ggplot2::theme_minimal()) +
  ggplot2::labs(color = "Referral")
```

Using the results of your final Cox regression model and your predicted survival functions to write a high-level business summary of what predictor or predictors you believe to be the most important to focus on to reduce attrition. 
